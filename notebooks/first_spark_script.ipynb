{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4398ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612ebbd",
   "metadata": {},
   "source": [
    "### Create SparkContext object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8213eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/20 17:37:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# If we are running this on a cluster - master should be set with the master_name,\n",
    "# which would usually by YARN or mesos, depending on the cluster type\n",
    "\n",
    "# Here, in local[x], x denotes the number of partitions should be created when using RDD,\n",
    "# DataFrame and Dataset. \n",
    "\n",
    "# Ideally x => number of CPU cores.\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"sid_spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd814254",
   "metadata": {},
   "source": [
    "### Create RDD from Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc2789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an RDD from Python list\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79efbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d177b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in RDD: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in RDD:\", rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253f6ac",
   "metadata": {},
   "source": [
    "### Manually set the number of partitions in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602913d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in RDD: 5\n"
     ]
    }
   ],
   "source": [
    "# To specify manually how many partitions to use\n",
    "rdd_partitioned = spark.sparkContext.parallelize([1,2,3,4,5], 5)\n",
    "\n",
    "print(\"Number of partitions in RDD:\", rdd_partitioned.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d98ec2",
   "metadata": {},
   "source": [
    "### Create empty RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7923f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty RDD with no partition\n",
    "rdd_empty = spark.sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9424005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty RDD with manual partition\n",
    "rdd_empty_partitioned = spark.sparkContext.parallelize([], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fec24",
   "metadata": {},
   "source": [
    "### Repartition and Coalesce\n",
    "\n",
    "Repartitioning and coalesce changes the number of partitions in the RDD to the number specified.\n",
    "Repartition shuffles the entire data, but coalesce will shuffle only the minimum required number.\n",
    "\n",
    "For e.g., in an RDD with 4 partitions:\n",
    "\n",
    "`repartition(2)`: shuffles all 4 partitions and creates 2 new partitions\n",
    "\n",
    "`coalesce(2)`: shuffles `4-2=2` partitions and gives resultant 2 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd9f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "repart_rdd = rdd_partitioned.repartition(4)\n",
    "coal_rdd = rdd_partitioned.coalesce(2)\n",
    "\n",
    "print(\"Number of partitions:\", repart_rdd.getNumPartitions())\n",
    "print(\"Number of partitions:\", coal_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b16063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
