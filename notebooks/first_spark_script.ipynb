{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4398ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612ebbd",
   "metadata": {},
   "source": [
    "### Create SparkContext object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8213eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/23 21:02:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# If we are running this on a cluster - master should be set with the master_name,\n",
    "# which would usually by YARN or mesos, depending on the cluster type\n",
    "\n",
    "# Here, in local[x], x denotes the number of partitions should be created when using RDD,\n",
    "# DataFrame and Dataset. \n",
    "\n",
    "# Ideally x => number of CPU cores.\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"sid_spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd814254",
   "metadata": {},
   "source": [
    "### Create RDD from Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc2789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an RDD from Python list\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79efbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d177b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in RDD: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions in RDD:\", rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253f6ac",
   "metadata": {},
   "source": [
    "### Manually set the number of partitions in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602913d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in RDD: 5\n"
     ]
    }
   ],
   "source": [
    "# To specify manually how many partitions to use\n",
    "rdd_partitioned = spark.sparkContext.parallelize([1,2,3,4,5], 5)\n",
    "\n",
    "print(\"Number of partitions in RDD:\", rdd_partitioned.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d98ec2",
   "metadata": {},
   "source": [
    "### Create empty RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7923f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty RDD with no partition\n",
    "rdd_empty = spark.sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9424005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty RDD with manual partition\n",
    "rdd_empty_partitioned = spark.sparkContext.parallelize([], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fec24",
   "metadata": {},
   "source": [
    "### Repartition and Coalesce\n",
    "\n",
    "Repartitioning and coalesce changes the number of partitions in the RDD to the number specified.\n",
    "Repartition shuffles the entire data, but coalesce will shuffle only the minimum required number.\n",
    "\n",
    "For e.g., in an RDD with 4 partitions:\n",
    "\n",
    "`repartition(2)`: shuffles all 4 partitions and creates 2 new partitions\n",
    "\n",
    "`coalesce(2)`: shuffles `4-2=2` partitions and gives resultant 2 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd9f3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "Number of partitions: 2\n"
     ]
    }
   ],
   "source": [
    "repart_rdd = rdd_partitioned.repartition(4)\n",
    "coal_rdd = rdd_partitioned.coalesce(2)\n",
    "\n",
    "print(\"Number of partitions:\", repart_rdd.getNumPartitions())\n",
    "print(\"Number of partitions:\", coal_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98bb722",
   "metadata": {},
   "source": [
    "### Persist/Cache RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d31d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "rdd_persisted = rdd.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4e8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_unpersisted = rdd_persisted.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789c6e6",
   "metadata": {},
   "source": [
    "### Shared Variables\n",
    "\n",
    "#### 1. Broadcast variables\n",
    "\n",
    "Broadcast variables are useful to cache some common lookup data in all the worker nodes. They are sent to the executors when they are first used. Important thing is that the values can be read from the executors but cannot be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b27bfa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19ef69",
   "metadata": {},
   "source": [
    "#### 2. Accumulator variables\n",
    "\n",
    "These are used for some accumulating functionality - like count or sum, i.e., each executor/task can write or update accumulator variables - but they cannot read it. The value of accumulator variables are available only to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b4a43ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x: accum.add(x))\n",
    "print(accum.value) # access by the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1ca63",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68dac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b512239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b7b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
